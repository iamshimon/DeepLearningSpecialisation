{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sigmoid function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_func(x):\n",
    "    \n",
    "    sig = 1/(1+np.exp(-x))\n",
    "\n",
    "    return sig\n",
    "\n",
    "x=np.array([1, 2, 3])\n",
    "print(sigmoid_func(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid derivative - to computer the gradients to optimise loss functions for back propogation\n",
    "\n",
    "def sigmoid_derivative_func(x):   \n",
    "    s= sigmoid_func(x)\n",
    "    ds = s*(1-s)\n",
    "\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image reshape\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def image2vector(img):\n",
    "    \n",
    "    v= img.reshape((img.shape[0]*img.shape[1]*img.shape[2], 1))\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "img= Image.open(r'cat.png')\n",
    "img = np.array(img)\n",
    "print(image2vector(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Rows - gradient descent converges faster after normalization\n",
    "\n",
    "def normalizeRows(x):\n",
    "\n",
    "    norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    x_norm = x/norm\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "x = np.array([[1,2,3],\n",
    "            [2, 6, 4]])\n",
    "print(normalizeRows(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax implementation\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "\n",
    "    x_exp = np.exp(x)\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True)\n",
    "    x_sum = np.sum(x_exp,axis=1, keepdims=True)\n",
    "    s = x_exp/x_sum\n",
    "\n",
    "    return s\n",
    "\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "#Implement L1 loss and L2 loss\n",
    "\n",
    "def L1(yhat, y):\n",
    "\n",
    "    loss = sum(abs(yhat-y))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def L2(yhat, y):\n",
    "\n",
    "    x = yhat - y\n",
    "    loss = np.dot(x,x)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
